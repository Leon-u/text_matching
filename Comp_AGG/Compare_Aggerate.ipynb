{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "improving-orientation",
   "metadata": {},
   "source": [
    "# compare-Aggregate模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "roman-termination",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "400001\n",
      "将'the of to .'文本转化为id序列：1 4 5 3 id_list转化为文本为：he is eating bread 将'the of to .'文本转化为嵌入向量为：\n",
      "[-7.1549e-02  9.3459e-02  2.3738e-02 -9.0339e-02  5.6123e-02  3.2547e-01\n",
      " -3.9796e-01 -9.2139e-02  6.1181e-02 -1.8950e-01  1.3061e-01  1.4349e-01\n",
      "  1.1479e-02  3.8158e-01  5.4030e-01 -1.4088e-01  2.4315e-01  2.3036e-01\n",
      " -5.5339e-01  4.8154e-02  4.5662e-01  3.2338e+00  2.0199e-02  4.9019e-02\n",
      " -1.4132e-02  7.6017e-02 -1.1527e-01  2.0060e-01 -7.7657e-02  2.4328e-01\n",
      "  1.6368e-01 -3.4118e-01 -6.6070e-02  1.0152e-01  3.8232e-02 -1.7668e-01\n",
      " -8.8153e-01 -3.3895e-01 -3.5481e-02 -5.5095e-01 -1.6899e-02 -4.3982e-01\n",
      "  3.9004e-02  4.0447e-01 -2.5880e-01  6.4594e-01  2.6641e-01  2.8009e-01\n",
      " -2.4625e-02  6.3302e-01 -3.1700e-01  1.0271e-01  3.0886e-01  9.7792e-02\n",
      " -3.8227e-01  8.6552e-02  4.7075e-02  2.3511e-01 -3.2127e-01 -2.8538e-01\n",
      "  1.6670e-01 -4.9707e-03 -6.2714e-01 -2.4904e-01  2.9713e-01  1.4379e-01\n",
      " -1.2325e-01 -5.8178e-02 -1.0290e-03 -8.2126e-02  3.6935e-01 -5.8442e-04\n",
      "  3.4286e-01  2.8426e-01 -6.8599e-02  6.5747e-01 -2.9087e-02  1.6184e-01\n",
      "  7.3672e-02 -3.0343e-01  9.5733e-02 -5.2860e-01 -2.2898e-01  6.4079e-02\n",
      "  1.5218e-02  3.4921e-01 -4.3960e-01 -4.3983e-01  7.7515e-01 -8.7767e-01\n",
      " -8.7504e-02  3.9598e-01  6.2362e-01 -2.6211e-01 -3.0539e-01 -2.2964e-02\n",
      "  3.0567e-01  6.7660e-02  1.5383e-01 -1.1211e-01 -9.1540e-02  8.2562e-02\n",
      "  1.6897e-01 -3.2952e-02 -2.8775e-01 -2.2320e-01 -9.0426e-02  1.2407e+00\n",
      " -1.8244e-01 -7.5219e-03 -4.1388e-02 -1.1083e-02  7.8186e-02  3.8511e-01\n",
      "  2.3334e-01  1.4414e-01 -9.1070e-04 -2.6388e-01 -2.0481e-01  1.0099e-01\n",
      "  1.4076e-01  2.8834e-01 -4.5429e-02  3.7247e-01  1.3645e-01 -6.7457e-01\n",
      "  2.2786e-01  1.2599e-01  2.9091e-02  3.0428e-02 -1.3028e-01  1.9408e-01\n",
      "  4.9014e-01 -3.9121e-01 -7.5952e-02  7.4731e-02  1.8902e-01 -1.6922e-01\n",
      " -2.6019e-01 -3.9771e-02 -2.4153e-01  1.0875e-01  3.0434e-01  3.6009e-02\n",
      "  1.4264e+00  1.2759e-01 -7.3811e-02 -2.0418e-01  8.0016e-03  1.5381e-01\n",
      "  2.0223e-01  2.8274e-01  9.6206e-02 -3.3634e-01  5.0983e-01  3.2625e-01\n",
      " -2.6535e-01  3.7400e-01 -3.0388e-01 -4.0033e-01 -4.2910e-02 -6.7897e-02\n",
      " -2.9332e-01  1.0978e-01 -4.5365e-02  2.3222e-01 -3.1134e-01 -2.8983e-01\n",
      " -6.6687e-01  5.3097e-01  1.9461e-01  3.6670e-01  2.6185e-01 -6.5187e-01\n",
      "  1.0266e-01  1.1363e-01 -1.2953e-01 -6.8246e-01 -1.8751e-01  1.4760e-01\n",
      "  1.0765e+00 -2.2908e-01 -9.3435e-03 -2.0651e-01 -3.5225e-01 -2.6720e-01\n",
      " -3.4307e-03  2.5906e-01  2.1759e-01  6.6158e-01  1.2180e-01  1.9957e-01\n",
      " -2.0303e-01  3.4474e-01 -2.4328e-01  1.3139e-01 -8.8767e-03  3.3617e-01\n",
      "  3.0591e-02  2.5577e-01]\n",
      "[ 5.2924e-02  2.5427e-01  3.1353e-01 -3.5613e-01  2.9629e-02  5.1034e-01\n",
      " -1.0716e-01  1.5195e-01  5.7698e-02  6.1490e-02  6.1160e-02  3.9911e-01\n",
      " -2.9018e-04  3.1978e-01  4.3257e-01 -1.4708e-01  5.4842e-02  2.7079e-01\n",
      " -1.4051e-01 -3.0101e-01  1.6313e-01  3.0013e+00  2.2231e-01 -1.4279e-01\n",
      "  8.3705e-02  8.9866e-02 -5.2706e-01 -8.9661e-02  2.7311e-01  3.1413e-01\n",
      " -4.0810e-02  6.0557e-02 -4.2656e-02  2.4178e-01 -2.9187e-01  2.2575e-01\n",
      " -6.2980e-01 -1.4641e-01 -2.2429e-01 -5.6621e-02 -1.7776e-01 -6.4269e-01\n",
      "  5.1626e-01  2.2305e-01  1.2124e-01  4.8074e-01  4.1743e-01  5.4805e-01\n",
      "  4.0955e-01  4.2407e-01  4.9906e-02 -3.2574e-01  4.6298e-01  1.9245e-01\n",
      "  2.8143e-01  2.9660e-01  6.3593e-02 -1.1906e-01 -1.5016e-01 -4.9840e-02\n",
      "  4.0675e-01  1.0675e-02 -6.9127e-01  4.8729e-02  2.6391e-01  3.0961e-01\n",
      " -1.1921e-01  2.5548e-01 -2.8219e-01 -3.7413e-02  3.6461e-01  2.7129e-02\n",
      "  2.0786e-01  5.3325e-01  5.0148e-01  7.2381e-01  6.5292e-02 -7.8716e-02\n",
      " -1.0537e-01 -8.0810e-02 -2.0960e-01  4.0902e-02 -8.8101e-01  2.4715e-01\n",
      "  1.6146e-01  1.0361e-01  1.9705e-01 -2.7365e-01  8.9902e-01 -2.9981e-01\n",
      "  3.6165e-02  4.1238e-02  6.0105e-01 -1.8911e-01 -4.3887e-01 -1.4097e-01\n",
      "  4.4073e-01 -1.9999e-01  2.8834e-01 -2.5458e-01 -1.0985e-01 -2.7379e-03\n",
      "  9.1735e-02  1.7021e-01 -1.6305e-01 -5.7439e-01  3.7063e-01  1.7262e+00\n",
      " -2.4656e-01  5.1681e-01 -1.5355e-01 -1.5553e-01  1.9783e-02  1.8030e-01\n",
      "  3.8178e-01  9.4443e-02 -5.5158e-01 -2.0242e-01 -4.3860e-01 -4.2108e-01\n",
      "  2.7525e-01  5.8977e-01  2.6655e-02  1.6401e-01  1.3893e-01 -6.8692e-01\n",
      "  5.1071e-01  2.9278e-01  2.2041e-02 -1.8156e-01 -6.4905e-01  1.6923e-01\n",
      " -1.0590e-02  2.1785e-01 -2.7242e-01  2.7967e-01  1.3950e-01 -7.0559e-01\n",
      " -2.6034e-01 -4.4017e-01  1.5303e-01  1.9693e-01 -9.6838e-02  1.4827e-01\n",
      "  1.1294e+00 -3.1267e-01  9.9916e-03 -4.8623e-01  8.0584e-02  3.5608e-01\n",
      " -1.9925e-01  1.9306e-01 -2.0040e-01 -4.4194e-01  7.5766e-01  2.4487e-01\n",
      " -1.8903e-01  2.6653e-01 -2.1339e-01 -5.4083e-01  4.0532e-01 -2.7960e-02\n",
      " -1.3398e-01 -1.1086e-01  5.9506e-02  2.4052e-01 -5.9739e-01 -2.4069e-03\n",
      " -1.8593e-01  1.0420e+00 -1.2969e-01  2.0813e-01  3.3305e-01 -1.2780e-01\n",
      "  8.5662e-02 -7.6422e-02  3.1407e-01 -2.3784e-01 -5.4838e-02  1.1369e-02\n",
      "  8.4500e-01 -3.4165e-01  9.3983e-02  8.2445e-02 -2.7777e-01 -4.4226e-01\n",
      " -6.3078e-02  3.7274e-01  5.4468e-02  2.4197e-01 -4.0886e-02  3.8940e-01\n",
      " -1.0509e-01  2.3372e-01  9.6027e-02 -3.0324e-01  2.4488e-01 -8.6254e-02\n",
      " -4.1917e-01  4.6496e-01]\n",
      "[ 5.7346e-01  5.4170e-01 -2.3477e-01 -3.6240e-01  4.0370e-01  1.1386e-01\n",
      " -4.4933e-01 -3.0991e-01 -5.3411e-03  5.8426e-01 -2.5956e-02  4.9393e-01\n",
      " -3.7209e-02 -2.8428e-01  9.7696e-02 -4.8907e-01  2.6027e-02  3.7649e-01\n",
      "  5.7788e-02 -4.6807e-01  8.1288e-02  3.2825e+00 -6.3690e-01  3.7956e-01\n",
      "  3.8167e-03  9.3607e-02 -1.2855e-01  1.7380e-01  1.0522e-01  2.8648e-01\n",
      "  2.1089e-01 -4.7076e-01  2.7733e-02 -1.9803e-01  7.6328e-02 -8.4629e-01\n",
      " -7.9708e-01 -3.8743e-01 -3.0422e-02 -2.6849e-01  4.8585e-01  1.2895e-01\n",
      "  3.8354e-01  3.8722e-01 -3.8524e-01  1.9075e-01  4.8998e-01  1.3278e-01\n",
      "  1.0792e-02  2.6770e-01  1.7812e-01 -1.1433e-01 -3.3494e-01  8.7306e-01\n",
      "  7.5875e-01 -3.0378e-01 -1.5626e-01  1.2085e-03  2.3322e-01  2.7953e-01\n",
      " -1.8494e-01 -1.4146e-01 -1.8969e-01 -3.8386e-02  3.5874e-01  6.5513e-02\n",
      "  6.0565e-02  6.6339e-01 -8.3252e-02  6.5163e-02  5.1761e-01  1.6171e-01\n",
      "  4.6011e-01  1.6388e-01 -1.2399e-01  3.1122e-01 -1.5412e-01 -1.0917e-01\n",
      " -4.2551e-01  1.1418e-01  2.5137e-01 -5.6158e-02 -2.5927e-01  2.8163e-01\n",
      " -1.8094e-02  1.6065e-01 -4.8506e-01 -9.8903e-01  2.5022e-01 -1.6736e-01\n",
      "  4.1474e-01  1.7701e-01  4.2407e-01  1.1088e-01 -1.8360e-01 -1.2410e-01\n",
      " -3.4780e-01  9.9078e-02 -2.2381e-01 -1.1245e-01 -2.1156e-01  3.0706e-03\n",
      " -2.3607e-01  2.7261e-02  3.6430e-01  3.9922e-02 -1.8369e-01  1.2266e+00\n",
      " -7.7640e-01 -6.6225e-01  1.5724e-02 -1.4969e-01  8.4649e-02  2.6814e-01\n",
      " -1.6765e-01 -3.1942e-01  2.8494e-01 -7.0000e-02  1.2010e-02 -1.2219e-01\n",
      "  5.6310e-01 -3.2000e-01  5.0109e-01 -1.0209e-01  4.6575e-01 -7.1542e-01\n",
      "  1.7293e-01  5.8259e-01  7.8384e-02 -3.3844e-02 -2.5129e-01  3.6503e-01\n",
      "  3.1578e-02 -6.5778e-01  5.4750e-02  8.7189e-01  1.2455e-01 -4.5877e-01\n",
      " -2.6965e-01 -4.6779e-01 -2.8578e-03  1.7810e-01  6.3969e-01  1.3995e-01\n",
      "  9.7596e-01  1.1836e-01 -6.3904e-01 -1.5416e-01  6.5262e-02  2.4329e-01\n",
      "  6.6476e-01  2.5069e-01 -1.0252e-01 -3.2839e-01 -8.5559e-02 -1.2774e-02\n",
      " -1.9431e-01  5.6139e-01 -3.5733e-01 -2.0344e-01 -1.2413e-01 -3.4431e-01\n",
      " -2.3296e-01 -2.1187e-01  8.5387e-02  7.0063e-02 -1.9803e-01 -2.6023e-02\n",
      " -3.9037e-01  8.0002e-01  4.0577e-01 -7.9863e-02  3.5263e-01 -3.4043e-01\n",
      "  3.9676e-01  2.2862e-01 -3.5028e-01 -4.7344e-01  5.9742e-01 -1.1657e-01\n",
      "  1.0552e+00 -4.1570e-01 -8.0552e-02 -5.6571e-02 -1.6622e-01  1.9274e-01\n",
      " -9.5175e-02 -2.0781e-01  1.5620e-01  5.0231e-02 -2.7915e-01  4.3742e-01\n",
      " -3.1237e-01  1.3194e-01 -3.3278e-01  1.8877e-01 -2.3422e-01  5.4418e-01\n",
      " -2.3069e-01  3.4947e-01]\n",
      "[ 1.2289e-01  5.8037e-01 -6.9635e-02 -5.0288e-01  1.0503e-01  3.9945e-01\n",
      " -3.8635e-01 -8.4279e-02  1.2219e-01  8.0312e-02  3.2337e-01  4.7579e-01\n",
      " -3.8375e-02 -7.0900e-03  4.1524e-01  3.2121e-01 -2.1185e-01  3.6144e-01\n",
      " -5.5623e-02 -3.0512e-02  4.2854e-01  2.8547e+00 -1.4623e-01 -1.7557e-01\n",
      "  3.1197e-01 -1.3118e-01  3.3298e-02  1.3093e-01  8.9889e-02 -1.2417e-01\n",
      "  2.3396e-03 -6.8954e-02 -1.0754e-01 -1.1551e-01 -3.1052e-01 -1.2097e-01\n",
      " -4.6691e-01 -8.3600e-02 -3.7664e-02 -7.1779e-02 -1.1899e-01 -2.0381e-01\n",
      " -1.2424e-01  4.6339e-01 -1.9828e-01 -8.0365e-03  5.3718e-01  3.1739e-02\n",
      "  3.4331e-01  7.9704e-03  4.8744e-03  3.0592e-02 -1.7615e-01  8.2342e-01\n",
      " -1.3793e-01 -1.0075e-01 -1.2686e-01  7.4735e-02 -8.8719e-02 -4.2719e-02\n",
      "  7.6624e-02  8.9263e-02  6.4445e-02 -3.1958e-02  1.5254e-01 -1.0384e-01\n",
      "  7.6604e-02  3.4099e-01  2.4331e-01 -1.0452e-01  4.0714e-01 -1.8260e-01\n",
      " -4.0667e-02  5.0878e-01  8.0760e-02  2.2759e-01 -4.2162e-02 -1.8171e-01\n",
      " -9.5025e-02  3.0334e-02  8.8202e-02 -3.9843e-06 -3.9877e-03  1.5724e-01\n",
      "  3.3167e-01  8.4710e-02 -2.5919e-01 -4.1384e-01  2.9920e-01 -5.4255e-01\n",
      "  3.2129e-02  1.0030e-01  4.4202e-01  4.4682e-02 -9.0681e-02 -1.0481e-01\n",
      " -1.1860e-01 -3.1972e-01 -2.0790e-01 -4.0203e-02 -2.2988e-02  2.2824e-01\n",
      "  5.5238e-03  1.2568e-01 -1.4640e-01 -1.4904e-01 -1.1561e-01  1.0517e+00\n",
      " -1.9498e-01  8.3958e-02  4.4812e-02 -1.2965e-01 -9.3468e-02  2.1237e-01\n",
      " -8.8332e-02 -1.8680e-01  2.6521e-01  1.3097e-01 -4.8102e-02 -2.2467e-01\n",
      "  2.8412e-01  3.4907e-01  3.4833e-01  1.7877e-02  3.0504e-01 -8.3453e-01\n",
      "  4.8856e-02 -1.9330e-01  2.0764e-01 -4.9701e-01 -1.8747e-01 -7.6801e-02\n",
      "  1.5558e-01 -4.6844e-01  4.0944e-01  2.1386e-01  8.2392e-02 -2.6491e-01\n",
      " -2.1224e-01 -1.3293e-01  1.4738e-01 -1.4192e-01  1.8994e-01 -1.5587e-01\n",
      "  1.0738e+00  4.0789e-01 -2.7452e-01 -1.8431e-01  6.8679e-04 -8.7115e-02\n",
      "  1.9672e-01  4.0918e-01 -3.5462e-01 -6.3260e-02  4.4920e-01 -6.0568e-02\n",
      " -4.1636e-02  2.0531e-01  1.7025e-02 -5.8448e-01  7.5441e-02  8.2116e-02\n",
      " -4.6008e-01  1.2393e-02 -2.5310e-02  1.4177e-01 -9.2192e-02  3.4505e-01\n",
      " -5.2136e-01  5.7304e-01  1.1973e-02  3.3196e-02  2.9672e-01 -2.7899e-01\n",
      "  1.9979e-01  2.5666e-01  8.2079e-02 -7.8436e-02  9.3719e-02  2.4202e-01\n",
      "  1.3495e+00 -3.0434e-01 -3.0936e-01  4.2047e-01 -7.9068e-02 -1.4819e-01\n",
      " -8.9404e-02  6.6800e-02  2.2405e-01  2.7226e-01 -3.5236e-02  1.7688e-01\n",
      " -5.3600e-02  7.0031e-03 -3.3006e-02 -8.0021e-02 -2.4451e-01 -3.9174e-02\n",
      " -1.6236e-01 -9.6652e-02]\n",
      "将id_list转化为嵌入向量为：\n",
      "[ 1.0278e-01 -3.7982e-02 -3.4679e-01 -2.0236e-01 -1.0104e-01 -4.1614e-03\n",
      " -1.8122e-01  5.2666e-02  6.3239e-01 -1.7635e-02  2.9064e-01  5.5232e-01\n",
      " -1.8253e-01  3.2052e-02  1.0706e-01  2.8863e-01 -5.4650e-01  6.3784e-01\n",
      "  2.6307e-01 -3.5245e-01  7.8052e-01  3.2884e+00  1.3932e-01 -1.7640e-01\n",
      " -2.1195e-01  3.1297e-01 -3.5545e-01 -8.4663e-02  1.9614e-01 -3.6436e-01\n",
      " -4.8380e-01 -2.9026e-02  1.4678e-01 -3.7160e-01  3.5807e-02 -6.0569e-01\n",
      " -1.0662e+00 -2.4823e-01  8.6176e-02  4.4917e-01 -2.7971e-01  7.2874e-02\n",
      " -3.2838e-01  4.5463e-01 -8.7686e-02 -2.2879e-01  4.4790e-01  5.6725e-01\n",
      "  1.1629e-01  1.8263e-01  2.7651e-01  4.8955e-01 -4.7938e-02  6.4848e-01\n",
      "  8.7681e-02 -3.9107e-01  1.6413e-01 -2.0127e-01 -3.7540e-01  2.1114e-02\n",
      " -1.3349e-01 -3.7299e-01 -2.7203e-01  8.6235e-02  3.2202e-01  8.9046e-02\n",
      " -2.6556e-01  3.3787e-01 -1.4177e-02  3.1975e-01  3.2740e-01 -4.7988e-01\n",
      " -5.8957e-02 -1.8290e-02 -1.3548e-01 -2.7461e-01 -3.8017e-01 -3.1293e-01\n",
      "  7.3876e-02  2.5744e-03  4.2813e-01 -1.7042e-01 -2.0509e-01  2.3494e-01\n",
      "  1.6507e-01  1.5267e-01 -8.2546e-01 -3.7710e-01  3.0854e-01 -6.0148e-01\n",
      " -1.6621e-01 -1.3708e-01 -3.1724e-02  7.4703e-02 -4.8448e-01 -2.3030e-01\n",
      "  2.4285e-01 -1.3020e-01 -6.3010e-02  1.4626e-01 -8.1932e-03  2.0477e-02\n",
      " -6.5081e-02  3.8807e-01 -7.0858e-02 -2.6930e-01  1.4757e-01  1.0983e+00\n",
      " -1.8736e-01 -1.0725e-01  5.3126e-01 -3.7668e-01  2.4258e-01 -1.7912e-01\n",
      "  1.5185e-01 -1.6000e-01 -8.6678e-02  2.9508e-01 -3.6161e-01 -2.5959e-01\n",
      "  1.5685e-01  3.9739e-01  2.4662e-02  4.9537e-01  1.6396e-01 -3.6334e-01\n",
      "  1.3761e-01 -4.7526e-02  4.8767e-01 -6.0551e-01 -3.7744e-01 -2.7895e-01\n",
      "  3.0921e-01 -5.4864e-01  2.3541e-01  5.3319e-01  4.4727e-03 -1.9318e-01\n",
      " -3.3792e-01  3.2271e-02  4.3456e-01 -3.3200e-01 -1.6313e-01 -1.2138e-01\n",
      "  2.0354e+00  6.4063e-01  3.0325e-01 -1.5715e-01 -1.4365e-02  6.9193e-02\n",
      "  3.5589e-01 -4.3891e-02 -3.2990e-01  1.4328e-01 -2.0706e-01 -5.8879e-01\n",
      " -8.7851e-02 -1.5630e-01  1.3603e-01 -1.6010e-01 -2.6251e-01  3.0142e-01\n",
      " -2.9627e-02 -7.0453e-02 -2.4357e-01 -1.5967e-01  1.5829e-01  1.9095e-01\n",
      " -5.0924e-01  4.3491e-01  5.7935e-02 -2.0903e-01  4.7752e-01 -1.1511e-01\n",
      "  3.2314e-01 -6.1648e-02 -6.5868e-01 -1.8690e-01  1.0805e+00  6.3511e-02\n",
      "  9.2858e-01 -1.8946e-01 -5.0856e-01  1.8354e-01 -4.5380e-01 -4.7037e-01\n",
      " -4.2802e-01  4.9602e-03  1.5261e-01  1.1386e-02  4.1096e-01 -5.8602e-02\n",
      " -2.2928e-01  1.0317e-01  3.9274e-01  4.2591e-01 -2.5326e-01  3.4194e-01\n",
      "  2.3397e-01 -4.5398e-02]\n",
      "[ 0.32928    0.25526    0.26753   -0.084809   0.29764    0.062339\n",
      " -0.15475    0.17784    0.32328   -0.92752    0.15194    0.16324\n",
      " -0.10428   -0.026464   0.65971    0.14782    0.38623    0.25169\n",
      "  0.1261    -0.43138    0.28092    3.1604    -0.17565   -0.0032247\n",
      "  0.64389   -0.39697    0.18975    0.37999   -0.079175  -0.14781\n",
      " -0.072965   0.057247  -0.42314    0.4508    -0.097386  -0.47587\n",
      " -0.96599   -0.75595   -0.033932  -0.070886  -0.44828   -0.52094\n",
      " -0.1823     0.18582   -0.074273  -0.017871   0.16742    0.015459\n",
      "  0.3029    -0.1258     0.32418   -0.31263   -0.076832   0.051959\n",
      "  0.27242   -0.18285   -0.36479   -0.63562   -0.21685    0.035812\n",
      "  0.12485    0.37268   -0.16976   -0.094146  -0.16412   -0.10728\n",
      "  0.037866   0.1175    -0.15533    0.34062    0.58848    0.38992\n",
      " -0.54839    0.85013   -0.83728    0.15482   -0.37191   -0.65409\n",
      " -0.27631   -0.025224   0.075732  -0.23904   -0.18311   -0.084571\n",
      "  0.15492   -0.16317   -0.26499    0.056831   0.88287   -0.47655\n",
      "  0.25131   -0.09316    0.34377   -0.35863   -0.22855    0.11918\n",
      "  0.29661   -0.2536     0.049002  -0.21234    0.16237    0.53871\n",
      "  0.035344   0.39293   -0.29673   -0.72556   -0.27431    1.3469\n",
      " -0.19218    0.50534    0.028451  -0.32206    0.096035  -0.0083551\n",
      " -0.013107  -0.32444   -0.10163    0.031755  -0.63196   -0.21541\n",
      " -0.035609   0.31259    0.23988   -0.19056   -0.13086   -0.12644\n",
      "  0.48795   -0.13492   -0.41967    0.15904   -0.27921   -0.017258\n",
      "  0.2937     0.067436   0.085052   0.099394  -0.0055281  0.094985\n",
      "  0.11167    0.19749    0.2523     0.32205    0.42778   -0.03518\n",
      "  1.3291     0.005261   0.26769   -0.46168    0.1125     0.10111\n",
      " -0.31174    0.5458    -0.37363   -0.026133   0.99566   -0.15827\n",
      " -0.26202    0.17324    0.060104  -0.48004    0.23841   -0.21495\n",
      "  0.077693  -0.089078   0.12985   -0.174     -0.057151   0.48207\n",
      " -0.14668    0.26739   -0.33366    0.32552    0.6252    -0.30905\n",
      "  0.087737  -0.17204    0.28246   -0.037268   0.16007    0.30031\n",
      "  1.4061    -0.32169   -0.025792   0.037175   0.026222  -0.27671\n",
      "  0.051688  -0.058734  -0.23223   -0.10529   -0.40318   -0.22161\n",
      "  0.060587   0.091321  -0.21363    0.071634  -0.21331    0.074621\n",
      "  0.012001  -0.21952  ]\n",
      "[ 0.084182  -0.62674   -0.30609    0.13065   -0.23365   -0.37507\n",
      " -0.26987   -0.4667    -0.29157    0.42288   -0.09402    0.58047\n",
      "  0.43744    0.070361  -1.2072     1.0642    -0.0067643  0.18344\n",
      "  0.56351    0.29461    0.0018534  1.7315     0.48127    0.63467\n",
      "  0.31854   -0.021111  -0.061383   0.55872    0.10865   -0.32306\n",
      "  0.66433    0.1474    -0.097993  -0.33632    0.22136    0.25121\n",
      " -0.31199    0.13077    0.023843   0.73072    0.49971   -0.19968\n",
      "  0.29352    0.26296    0.030604   0.2507     0.76165   -0.042507\n",
      " -0.23598    0.81876   -0.2941     0.43207    0.031597   0.79336\n",
      " -0.29333   -0.7031     0.32115   -0.6126    -0.14018    0.48898\n",
      " -0.34928    0.098814  -0.69736    0.84597   -0.44913    0.23829\n",
      "  0.13618    0.036322   0.30951    0.41806    0.47479    0.42287\n",
      " -0.52375    0.1225    -0.42811   -0.058554  -0.1787    -0.2541\n",
      " -0.44581   -0.57875   -0.56351    0.19173    0.003646  -0.21379\n",
      "  0.082811  -0.2217     0.68467   -0.21389    0.47581   -0.96009\n",
      "  0.11401    0.61645    0.51136    1.1266     0.26941   -0.25943\n",
      " -0.16488   -0.346     -0.55966   -0.19561    0.35736   -0.12893\n",
      " -0.26916   -0.0391    -0.74557    0.14259    0.50571    0.81453\n",
      " -0.06188   -0.17038    0.43495    0.36257    0.14458   -0.19189\n",
      " -0.094588  -0.35486   -0.05149    0.038721  -0.84328   -0.2908\n",
      "  0.57329    0.30925   -0.82419   -0.25016    0.35937    0.15202\n",
      "  0.64373   -0.14534    0.16727    0.39865    0.039461  -0.028639\n",
      " -0.22367    0.75364    0.027177  -0.91962    0.11679    0.38691\n",
      "  0.0096648  0.29539    0.0087469 -0.10991   -0.088253  -0.25552\n",
      "  0.20646    0.99576    0.45974   -0.53169    0.38353   -0.66379\n",
      " -0.41469    1.0715    -0.46709    0.46921    0.48066    0.22265\n",
      "  0.6132     0.11015   -0.69317    0.42171   -0.040733   0.57204\n",
      "  0.094801   0.019407   0.21456   -0.40877   -0.63304    0.38908\n",
      " -0.74172    0.031881  -0.90146   -0.42636    0.028801   0.45243\n",
      " -0.011036   0.55723    0.11562   -0.025466   0.12184    0.45205\n",
      "  0.9451    -0.20992   -1.1082    -0.11661   -0.25699    0.29983\n",
      " -0.79439    0.15314    0.73531   -0.28241   -0.11507   -0.029954\n",
      " -0.046232   0.32038    0.61015   -0.20763   -0.42252   -0.42038\n",
      " -0.14596    0.11603  ]\n",
      "[ 0.54942    0.1625     0.37534    0.77585   -0.46447   -0.58681\n",
      " -0.73676   -0.38155    0.40982   -0.36744   -0.033875   0.24979\n",
      "  0.30254    1.0925    -1.0022     0.16617    0.34945   -0.20955\n",
      "  0.6672     0.48601   -0.33434    1.2564     0.17005   -0.47841\n",
      " -0.070053  -0.31884    0.6232    -0.13444   -0.70672    0.40393\n",
      "  0.78533    0.75616    0.042928   0.20537   -0.03715    0.12788\n",
      "  0.10231   -0.059529   0.056912  -0.12135    0.067614   0.77138\n",
      "  0.43462   -0.38983   -0.23805   -0.56042    0.72498    0.49939\n",
      " -0.16221    0.63375    0.13358    0.28451    0.12608    0.67093\n",
      "  0.2234    -0.1655    -0.67076    0.26071    0.7842    -0.19449\n",
      "  0.078822   0.14123    0.013915  -0.12071    0.011505   0.0027892\n",
      " -0.1549     0.30429   -0.31428   -0.73283    1.3175     0.54172\n",
      " -0.51441   -0.59867    0.045218   0.50037   -0.3465    -0.62901\n",
      " -0.48276   -0.44833   -0.10842   -0.82545   -0.34654   -0.34188\n",
      "  0.23359    0.27447   -0.088475  -0.13374   -0.077033  -0.86835\n",
      " -0.030642   0.38865   -0.44165    0.36087   -0.22312   -0.93208\n",
      "  0.21393   -0.42214    0.14004   -0.3111    -0.64585   -0.45803\n",
      "  0.53872    0.23149   -0.25631    0.89029    0.16388    0.97725\n",
      " -0.27816   -0.22985    0.2897     0.062368   0.6033    -0.036441\n",
      " -0.26342    0.16449    0.033121  -0.22032    0.10932    0.55239\n",
      "  0.29556    0.29107   -0.62463    0.43132    0.43222   -0.33734\n",
      "  0.58327   -0.47289   -0.13211    0.60316   -0.49042    0.22236\n",
      " -0.52128    0.7481    -1.0601    -0.76255    0.96272   -0.29285\n",
      " -0.50802    0.1775     0.024608  -0.91114    0.89396   -0.35038\n",
      "  0.16248    0.1604     0.050518  -1.0556     0.10031   -0.85127\n",
      " -0.43503    0.85287    0.025043  -0.14462    0.24425    0.40351\n",
      " -0.20764    0.3374     0.54974    0.31844   -0.077174   0.30172\n",
      "  0.35862    0.3165    -0.52106    0.058544  -0.86817    0.7909\n",
      " -0.38386    0.15404   -1.157     -0.42968   -0.2294     0.80086\n",
      "  0.23345    0.6446    -0.026177  -0.13457    0.15665    0.64804\n",
      "  0.70349   -0.40151   -0.58523    0.14016   -0.71282    0.030269\n",
      " -1.0198     0.52259    0.62557    0.46072   -0.11652    0.044819\n",
      " -0.42027   -0.19499    0.19389   -0.46471   -0.8247    -0.11936\n",
      " -0.050544   0.57478  ]\n",
      "[[5, 4, 15, 12, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]]\n",
      "[[42, 118, 2359, 2742, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 3, 0, 0, 0, 0, 0]]\n",
      "[[220, 220, 807, 2, 123], [47822, 0, 0, 29984, 0]]\n"
     ]
    }
   ],
   "source": [
    "from data_load import load_char_data, get_embed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "functioning-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "floral-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=400001\n",
    "embedding_size=200\n",
    "hidden_size=100\n",
    "classes=3\n",
    "\n",
    "maxq_len=20\n",
    "maxa_len=20\n",
    "window=3\n",
    "\n",
    "EPOCH=3\n",
    "BATCH_SIZE=2\n",
    "LR=0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "whole-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = './data/embed.pkl'\n",
    "with open(embed_file, 'rb') as f:\n",
    "    embed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-belly",
   "metadata": {},
   "source": [
    "# 模型结构\n",
    "## 预处理层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-maryland",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/预处理.png\"  width=\"400\" height=\"400\" align=\"left\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "democratic-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Preprocess, self).__init__()\n",
    "        # Parameter是可传递参数的\n",
    "        self.Wi = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bi = nn.Parameter(torch.randn(out_features))\n",
    "        \n",
    "        self.Wu = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bu = nn.Parameter(torch.randn(out_features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gate = torch.matmul(x, self.Wi)\n",
    "        # 把一个tensor变成和函数括号内一样形状的tensor\n",
    "        gate = torch.sigmoid(gate + self.bi.expand_as(gate))\n",
    "        out = torch.matmul(x, self.Wu)\n",
    "        out = torch.tanh(out + self.bu.expand_as(out))\n",
    "        \n",
    "        return gate*out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-harris",
   "metadata": {},
   "source": [
    "# 注意力层\n",
    "<img src=\"./imgs/注意力.png\"  width=\"400\" height=\"400\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "minus-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.Wg = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.bg = nn.Parameter(torch.randn(hidden_size))\n",
    "        \n",
    "    def forward(self, Q, A):\n",
    "        G = torch.matmul(Q, self.Wg)\n",
    "        G = G + self.bg.expand_as(G)\n",
    "        \n",
    "        G = torch.matmul(G, A.permute(0,2,1))\n",
    "        G = torch.softmax(G, dim=1)\n",
    "        H = torch.matmul(G.permute(0,2,1),Q)\n",
    "        return H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-medicaid",
   "metadata": {},
   "source": [
    "# 比较层\n",
    "<img src=\"./imgs/比较.png\"  width=\"400\" height=\"400\" align=\"left\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "positive-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compare(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Compare, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(2*hidden_size, hidden_size))\n",
    "        self.b = nn.Parameter(torch.randn(hidden_size))\n",
    "        \n",
    "    def forward(self, h, a):\n",
    "        sub = (h-a)*(h-a)\n",
    "        mult = h*a\n",
    "\n",
    "        T = torch.matmul( torch.cat([sub,mult],dim=2), self.W)\n",
    "        T = torch.relu(T+self.b.expand_as(T))\n",
    "        return T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "inside-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompAgg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompAgg, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, embedding_size)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embed))\n",
    "        self.preprocess = Preprocess(embedding_size, hidden_size)\n",
    "        self.attention = Attention()\n",
    "        self.compare = Compare()\n",
    "        self.aggregate = nn.Conv1d(in_channels=maxa_len, out_channels=window, kernel_size=3, stride=1, padding=1)\n",
    "        self.predict = nn.Linear(window*hidden_size, classes)\n",
    "        \n",
    "    def forward(self, Q, A):\n",
    "        # emb_q : batch_size, seq_len_q, embedding_size(200)\n",
    "        # emb_a : batch_size, seq_len_a, embedding_size(200)\n",
    "        emb_q = self.embedding(Q)\n",
    "        emb_a = self.embedding(A)\n",
    "        \n",
    "        # q_bar : batch_size, seq_len_q, hidden_size(100)\n",
    "        # a_bar : batch_size, seq_len_a, hidden_size(100)\n",
    "        q_bar = self.preprocess(emb_q)\n",
    "        a_bar = self.preprocess(emb_a)\n",
    "        \n",
    "        # H : batch_size, seq_len_a, hidden_size\n",
    "        H = self.attention(q_bar, a_bar)\n",
    "        \n",
    "        # T : batch_size, seq_len_a, hidden_size\n",
    "        T = self.compare(H, a_bar)\n",
    "        \n",
    "        # r : batch_size, window, hidden_size\n",
    "        r = self.aggregate(T)\n",
    "        r = r.view(-1, window*hidden_size)\n",
    "        \n",
    "        # r : batch, 3\n",
    "        out = self.predict(r)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "painted-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self, filepath, len_a, len_b):\n",
    "        self.path=filepath\n",
    "        self.a_index, self.b_index, self.label=load_char_data(filepath, len_a, len_b)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.a_index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.a_index[idx], self.b_index[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-jimmy",
   "metadata": {},
   "source": [
    "### 函数模型举例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dated-party",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   56,   462,  2123,     4,  6048,   777,   283,   536,   111,\n",
       "         2759,   550,  1465,  7814,     6, 14781,     0,     0,     0,\n",
       "            0,     0]),\n",
       " array([ 2927, 16789,    67,     4,  1992,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]),\n",
       " 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path='./data/SNLI/snli-dev.txt'\n",
    "train_data=myDataset(train_path,maxq_len,maxa_len)\n",
    "train_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cellular-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='./data/SNLI/snli-train.txt'\n",
    "test_path='./data/SNLI/snli-test.txt'\n",
    "\n",
    "\n",
    "\n",
    "train_data=myDataset(train_path,maxq_len,maxa_len)\n",
    "test_data=myDataset(test_path,maxq_len,maxa_len)\n",
    "train_loader=DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader=DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=torch.device('cpu')\n",
    "net=CompAgg().to(device)\n",
    "\n",
    "\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=LR)\n",
    "loss_func=torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for step,(text_a,text_b,label) in enumerate(train_loader):\n",
    "        #1、把索引转化为tensor变量，载入设备\n",
    "        a=text_a.to(device).long()\n",
    "        b=text_b.to(device).long()\n",
    "        l=torch.LongTensor(label).to(device)\n",
    "        \n",
    "        \n",
    "        #2、计算模型输出\n",
    "        out=net(a,b)\n",
    "\n",
    "        #3、预测结果传给loss\n",
    "        loss=loss_func(out,l)\n",
    "        \n",
    "        #4、固定格式\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            total=0\n",
    "            correct=0\n",
    "            for (test_a,test_b,test_l) in test_loader:\n",
    "                tst_a=test_a.to(device).long()\n",
    "                tst_b=test_b.to(device).long()\n",
    "                tst_l=torch.LongTensor(test_l).to(device)\n",
    "                out=net(tst_a,tst_b)\n",
    "                out=torch.argmax(out,dim=1).long()\n",
    "                if out.size()==tst_l.size():\n",
    "                    total+=tst_l.size(0)\n",
    "                    correct+=(out==tst_l).sum().item()\n",
    "                \n",
    "            print('[Epoch ~ Step]:',epoch+1,'~',step+1,'训练loss:',loss.item())\n",
    "            print('[Epoch]:',epoch+1,'测试集准确率: ',(correct*1.0/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-patrol",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
