{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worthy-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.data import Field, Example, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "associate-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0004\n",
    "hidden_size=100\n",
    "linear_size=100\n",
    "batch_size=100\n",
    "classes=3\n",
    "EPOCH=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "norwegian-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESIM(nn.Module):\n",
    "    def __init__(self, TEXT):\n",
    "        super(ESIM,self).__init__()\n",
    "        self.embedding = nn.Embedding(*TEXT.vocab.vectors.size())\n",
    "        '''\n",
    "        nn.Embedding(*TEXT.vocab.vectors.size())\n",
    "      ==nn.Embedding(*(36990,100))\n",
    "      ==nn.Embedding(36990,100)\n",
    "        '''\n",
    "        self.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        # 编码层双向LSTM\n",
    "        self.A_bilstm_input = nn.LSTM(TEXT.vocab.vectors.size()[1],hidden_size, batch_first=True,bidirectional=True)\n",
    "        self.B_bilstm_input = nn.LSTM(TEXT.vocab.vectors.size()[1],hidden_size, batch_first=True,bidirectional=True)\n",
    "        # 推理组合层双向LSTM\n",
    "        self.A_bilstm_infer = nn.LSTM(8*hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.B_bilstm_infer = nn.LSTM(8*hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(8*hidden_size, 2*hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*hidden_size, linear_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(linear_size, classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        emb_a = self.embedding(a)\n",
    "        emb_b=self.embedding(b)\n",
    "        \n",
    "        #输入编码层\n",
    "        a_bar,(h,c)=self.A_bilstm_input(emb_a)\n",
    "        b_bar,(h,c)=self.B_bilstm_input(emb_b)\n",
    "\n",
    "        e = torch.matmul(a_bar, b_bar.permute(0,2,1))\n",
    "        a_tilde = torch.matmul(torch.softmax(e, dim=2), b_bar)\n",
    "        b_tilde = torch.matmul(torch.softmax(e, dim=1), a_bar)\n",
    "        \n",
    "        # 矩阵拼接\n",
    "        ma = torch.cat([a_bar, a_tilde, a_bar-a_tilde, a_bar*a_tilde],dim=2)\n",
    "        mb=torch.cat([b_bar,b_tilde,b_bar-b_tilde,b_bar*b_tilde],dim=2)\n",
    "        \n",
    "        #推理组合层\n",
    "        va,(h,c)=self.A_bilstm_infer(ma)\n",
    "        vb,(h,c)=self.B_bilstm_infer(mb)\n",
    "        \n",
    "        va_avg = torch.mean(va, dim=1)\n",
    "        va_max = torch.max(va, dim=1)[0]\n",
    "        vb_avg = torch.mean(vb, dim=1)\n",
    "        vb_max = torch.max(vb, dim=1)[0]\n",
    "        v=torch.cat([va_avg,va_max,vb_avg,vb_max],dim=1)\n",
    "        \n",
    "        #输出预测层\n",
    "        out=self.linear(v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convinced-boating",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Env\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torchtext\\data\\field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "D:\\Env\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torchtext\\data\\example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "def text_tokenize(x):\n",
    "    return x.split()\n",
    "\n",
    "def label_tokenize(y):\n",
    "    return y\n",
    "\n",
    "def get_dataset(csv_data,text_field,label_field):\n",
    "    # csv_data为padas读取后的DataFrame\n",
    "    fields=[('sentence1',text_field),('sentence2',text_field),('gold_label',label_field)]\n",
    "    examples=[]\n",
    "    for text1,text2,label in zip(csv_data['sentence1'],csv_data['sentence2'],csv_data['gold_label']):\n",
    "        examples.append(data.Example.fromlist([text1,text2,label],fields))\n",
    "    return examples,fields\n",
    "\n",
    "#1.1 文本的预处理方法\n",
    "TEXT=data.Field(sequential=True,tokenize=text_tokenize,fix_length=40)\n",
    "\n",
    "#1.2 标签的预处理方法\n",
    "LABEL=data.Field(sequential=False,tokenize=label_tokenize,use_vocab=False)\n",
    "\n",
    "train_data=pd.read_csv('./data/SNLI/snli-train.txt',sep='\\t')\n",
    "test_data=pd.read_csv('./data/SNLI/snli-test.txt',sep='\\t')\n",
    "\n",
    "#2.1 获取example的列表\n",
    "train_examples,train_fields=get_dataset(train_data,TEXT,LABEL)\n",
    "test_examples,test_fields=get_dataset(test_data,TEXT,LABEL)\n",
    "\n",
    "#2.2 获取Dataset\n",
    "train=data.Dataset(train_examples, train_fields)\n",
    "test=data.Dataset(test_examples, test_fields)\n",
    "\n",
    "#3 导入预训练好的词向量\n",
    "vectors = Vectors(name='./data/glove.6B.300d.txt')\n",
    "TEXT.build_vocab(train,vectors=vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "european-essex",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Env\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torchtext\\data\\iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#4 构建迭代器\n",
    "train_iter=Iterator(train,batch_size=100,sort=False,device=torch.device('cuda:0'),repeat=False)\n",
    "test_iter=Iterator(test, batch_size=100,device=torch.device('cuda:0'), sort=False,repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fallen-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 有gpu用gpu，否则cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net=ESIM(TEXT).to(device)\n",
    "\n",
    "#6、定义优化方式和损失函数\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=lr)\n",
    "loss_func=torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for step,batch in enumerate(train_iter):\n",
    "        #1、把索引转化为tensor变量，载入设备\n",
    "        a=batch.sentence1.t()\n",
    "        b=batch.sentence2.t()\n",
    "        l=batch.gold_label\n",
    "        \n",
    "        #2、计算模型输出\n",
    "        out=net(a,b)\n",
    "\n",
    "        #3、预测结果传给loss\n",
    "        loss=loss_func(out,l)\n",
    "        \n",
    "        #4、固定格式\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            total=0\n",
    "            correct=0\n",
    "            for batch in test_iter:\n",
    "                tst_a=batch.sentence1.t()\n",
    "                tst_b=batch.sentence2.t()\n",
    "                tst_l=batch.gold_label\n",
    "                out=net(tst_a,tst_b)\n",
    "                out=torch.argmax(out,dim=1).long()\n",
    "                if out.size()==tst_l.size():\n",
    "                    total+=tst_l.size(0)\n",
    "                    correct+=(out==tst_l).sum().item()\n",
    "                \n",
    "            print('[Epoch ~ Step]:',epoch+1,'~',step+1,'训练loss:',loss.item())\n",
    "            print('[Epoch]:',epoch+1,'测试集准确率: ',(correct*1.0/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-concentration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
